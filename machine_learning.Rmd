---
title: "Machine Learning Assignment"
output: pdf_document
---


Prediction of the manner in which a human subject performed a dumbbell (1.25kg) exercise based on accelerometer measurements.
-------------------------------------------------------

Synopsis
------------
The goal of this project is to predict the manner in which  a human subject did the dumbbell (1.25kg) exercise. We used th "classe" variable in the training set. We created a training and a test data to do the analysis. Our prediction model was build using random forest and classsification trees. We also used our model to predict 20 different test cases. 

Data
------
The data for this project come from this source: [Human Activity Recognition Website](http://groupware.les.inf.puc-rio.br/har). Data was provided in two files the **[`pml-training.csv`](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)** and the **[`pml-testing.csv`](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)**. Our variable of interest was the `classe` ( A, B, C, D or E )  variable, our model aims to predict which class each human belonged.


Loading and Summary of the Data
---------------------------------

```{r setoptions, echo=F }
library('knitr')
opts_chunk$set(echo=F, results="show" , include=F  )
```

```{r packages,warning=F,include=F , echo=TRUE ,  results="show"}
library(caret)
library(rattle)
library(randomForest)
library(plyr)
library(dplyr)
library(xtable)
#load the dataset
#creatind the data folder if it doesnt exist
if (!file.exists("data")) {
  dir.create("data")
}

#downloadTheData
if (!file.exists("data/pml-training.csv")){
  fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(fileUrl, destfile = "data/pml-training.csv")
}else {message("Data Already downloaded")}

if (!file.exists("data/pml-testing.csv")){
  fileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(fileUrl, destfile = "data/pml-testing.csv")
}else {message("Data Already downloaded")}


#load the data to R
pml_training <- read.table("data/pml-training.csv", header = T , sep = ",")
pml_testing <-  read.table("data/pml-testing.csv", header = T , sep = ",")


set.seed(975)

#colnames in training data
colnames_training <- colnames(pml_training)
colnames_predictors <- colnames_training[ grep(".*belt.*|.*arm.*|.*dumbbell.*",
                                              colnames(pml_training)) ]

cols_no_NA <- sapply(pml_training[,colnames_predictors],
                    function(x) !any(is.na(x)))

# reduce predictors to columns free of NA values
colnames_predictors <- colnames_predictors[cols_no_NA]

# eliminate predictor columns with "#DIV/0!"
cols_no_DIV0 <- sapply(pml_training[,colnames_predictors],
                      function(x) length(grep(".*DIV/0.*",x))==0)
colnames_predictors <- colnames_predictors[cols_no_DIV0]

#the count of values with predictors
length(colnames_predictors)


```

After dwonloading the  testing and training data files, both data sets had `r length(pml_testing)` variables. For our analysis only columns related to belt, arm or dumbbell measurements were kept as predictors.  Additionally, any columns containing `NA` or `#DIV/0!` values were excluded as predictors and these constraints yielded `r length(colnames_predictors)` predictors.




### 1. Transform Data
The training data set was then partitioned into `training_train` \- 60% and `training_test` \- 40% for building our model on using a . A feature plot was then prepared to  observe for paterns and interdependence between predictors. 

```{r dataManagement , echo=TRUE ,warning=F,include=T}
# dataframes containing only the intended predictors
pmlTrain <- pml_training[,colnames_predictors]
pmlTrain$classe <- pml_training$classe
pmlTest <-  pml_testing[,c(colnames_predictors)]
pmlTest$classe <- pml_testing$classe

#in training data create a training and testing data
inTrain <- createDataPartition(y=pmlTrain$classe,
                               p=0.6, list=FALSE)
pmlTrain_train <- pmlTrain[inTrain,]
pmlTest_test <- pmlTrain[-inTrain,]

#in training data create a training and testing data
inTrain <- createDataPartition(y=pmlTrain$classe,
                               p=0.6, list=FALSE)
pmlTrain_train <- pmlTrain[inTrain,]
pmlTest_test <- pmlTrain[-inTrain,]

#columns related to be used as predictors
is_Roll <- (substr(names(pmlTrain_train),1,4) == "roll")
is_Yaw <- (substr(names(pmlTrain_train),1,3) == "yaw")
is_Pitch <- (substr(names(pmlTrain_train),1,5) == "pitch")


```


```{r featurePlot , echo=TRUE ,warning=F,include=T}
fplot1 <- featurePlot(x=pmlTrain_train[,names(pmlTrain_train)[is_Roll]], y= pmlTrain_train$classe, plot="pairs")
print(fplot1)

```


### 2. Building the classification tree model
We created a classification tree usinf the `rpart` method in the `carat` package.

```{r classificationTree , echo=TRUE ,warning=F,include=T}
#create a classification tree
modFit1 <- train(classe ~ .,method="rpart",data=pmlTrain_train)
print(modFit1$finalModel)
```


```{r classificationTreeb , echo=F ,warning=F,include=F}
## Plot tree
plot(modFit1$finalModel, uniform=TRUE, 
     main="Classification Tree")
text(modFit1$finalModel, use.n=TRUE, all=TRUE, cex=.8)



## Prettier plots
##not clear
fplot1 <- fancyRpartPlot(modFit1$finalModel)

### estimate the prediction
pred1 <- predict(modFit1,newdata=pmlTrain_train)
tab1 <- table(pmlTrain_train$classe,pred1)

## miss classification rate
mer <- 1-sum(diag(tab1))/sum(tab1)
```

The misclassification error rate observed was `r round(mer,2)`  This value is close to 0.5 then `no purity` thus we decided to use a random forest model
The prediction table was

```{r echo=T,error=F,warning=F , include=TRUE}
#kable(tab1, format = "markdown")
tab1 <- table(pmlTrain_train$classe,pred1)
print(tab1)
```

The classification tree produced was

```{r fancyplots , echo=TRUE ,warning=F,include=T, ref.label=F}
fplot1 <- fancyRpartPlot(modFit1$finalModel)
print(fplot1)
```

**Conclusion:**  with default settings `rpart` model seems not satisfactory

### 3. Building the random forest model
After the classification tree model,  we decided to build a random forest model. The generic plot using model from randomForest is as shown below 



```{r rforest1 , echo=T ,warning=F,include=T}
# create a Random Forest model
modFit2 <- randomForest(classe ~ ., data=pmlTrain_train)
# display model fit results
modFit2

# Generic plot using model from randomForest
plot(modFit2, log="y",
     main="Estimated Out-of-Bag (OOB) Error and Class Error of Random Forest Model")
legend("top", colnames(modFit2$err.rate), col=1:6, cex=0.8, fill=1:6)

```

We also did a dot plot for the variables of importance 

```{r rforest3 , echo=T ,warning=F,include=T}
# Dotchart of variable importance as measured by randomForest
varImpPlot(modFit2, main="Variable Importance in the Random Forest Model")
#plot1 <- varImp(modFit2, scale = FALSE)
#plot(plot1, top = 20)

```


```{r rforest4 , echo=F ,warning=F,include=F}
# estimate out of sample error
pred_out_of_sample <- predict(modFit2, newdata=subset(pmlTest_test, select=-classe))

# out of sample confusion matrix
confusion_matrix <- table(pred_out_of_sample, pmlTest_test$classe)
confusion_matrix
# estimated out of sample error rate
out_of_sample_error_rate = 1.00 - sum(diag(confusion_matrix)) / sum(confusion_matrix)
out_of_sample_error_rate

pred2 <- predict(modFit2,newdata=pmlTest_test)
tab2 <- table(pmlTest_test$classe,pred2)

## miss classification rate
mer2 <- 1-sum(diag(tab2))/sum(tab2)

# predict using pml-test data
pred <- predict(modFit2, newdata=pml_testing)

```

The random forest model had convincing results. The model had a misclassification error rate of `r mer2` and the out of sample error rate of `r out_of_sample_error_rate`.

```{r echo=T,error=F,warning=F, include=TRUE}
#kable( tab2 , format = "markdown")
tab2 <- table(pmlTest_test$classe,pred2)
print(tab2)
```


Writing the ouputs

```{r outputs, echo=T,error=F,warning=F, include=TRUE}
# write prediction answers to files
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_", i, ".txt")
    write.table(x[i], file=filename, quote=FALSE, row.names=FALSE, col.names=FALSE)
  }
}
pml_write_files(pred)
```
